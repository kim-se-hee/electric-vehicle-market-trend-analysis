"""
RAG Tool
ë²¡í„° DB ê¸°ë°˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±
"""
from typing import List, Optional
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from config.settings import settings
import logging


class RAGTool:
    """RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logging.getLogger("RAGTool")
        self.embeddings = OpenAIEmbeddings(
            api_key=settings.OPENAI_API_KEY
        )
        self.vectorstore: Optional[FAISS] = None
        self.qa_chain: Optional[RetrievalQA] = None
        
    def build_vectorstore(self, documents: List[Document]) -> None:
        """
        ë¬¸ì„œë¡œë¶€í„° ë²¡í„° DB êµ¬ì¶•
        
        Args:
            documents: Document ë¦¬ìŠ¤íŠ¸
        """
        if not documents:
            self.logger.warning("âš ï¸  ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„° DBë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        self.logger.info(f"ğŸ”§ {len(documents)}ê°œ ë¬¸ì„œë¡œ ë²¡í„° DB êµ¬ì¶• ì¤‘...")
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        
        # QA ì²´ì¸ ìƒì„±
        llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            temperature=0.0,
            api_key=settings.OPENAI_API_KEY
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰
            ),
            return_source_documents=True
        )
        
        self.logger.info("âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ")
    
    def query(self, question: str) -> str:
        """
        RAG ì§ˆì˜
        
        Args:
            question: ì§ˆë¬¸
            
        Returns:
            ë‹µë³€
        """
        if not self.qa_chain:
            return "ë²¡í„° DBê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            result = self.qa_chain.invoke({"query": question})
            return result["result"]
            
        except Exception as e:
            self.logger.error(f"âŒ RAG ì§ˆì˜ ì‹¤íŒ¨: {e}")
            return f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def similarity_search(self, query: str, k: int = 5) -> List[Document]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            return []
        
        return self.vectorstore.similarity_search(query, k=k)
    
    def save_vectorstore(self, path: str) -> None:
        """ë²¡í„°ìŠ¤í† ì–´ ì €ì¥"""
        if self.vectorstore:
            self.vectorstore.save_local(path)
            self.logger.info(f"ğŸ’¾ ë²¡í„° DB ì €ì¥: {path}")
    
    def load_vectorstore(self, path: str) -> None:
        """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            self.vectorstore = FAISS.load_local(
                path, 
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # QA ì²´ì¸ ì¬ìƒì„±
            llm = ChatOpenAI(
                model=settings.LLM_MODEL,
                temperature=0.0,
                api_key=settings.OPENAI_API_KEY
            )
            
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=self.vectorstore.as_retriever(
                    search_kwargs={"k": 5}
                ),
                return_source_documents=True
            )
            
            self.logger.info(f"ğŸ“‚ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")