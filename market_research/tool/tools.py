from typing import List, Dict, Optional
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain.tools import tool
from common.config.settings import settings  # .envì—ì„œ ê°€ì ¸ì˜¨ settings ì‚¬ìš©
import asyncio
import aiohttp
import os

os.environ["TAVILY_API_KEY"] = settings.TAVILY_API_KEY  # â† ì¶”ê°€
os.environ["USER_AGENT"] = "MyEVMarketAnalysisBot/1.0"  # â† ì¶”ê°€

class TavilySearchTool:
    """Tavily ê²€ìƒ‰ ë„êµ¬ ë˜í¼"""
    
    def __init__(self, max_results: int = 5):
        self.search = TavilySearchResults(
            max_results=max_results,
            search_depth="advanced",  # "basic" ë˜ëŠ” "advanced"
            include_answer=True,
            include_raw_content=True,
            include_images=False,
            api_key=settings.TAVILY_API_KEY  # âœ… .envì—ì„œ ê°€ì ¸ì˜´
        )
    
    def search_web(self, query: str) -> List[Dict]:
        """ì›¹ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            results = self.search.invoke({"query": query})
            
            # Tavily ê²°ê³¼ í¬ë§· ì •ë¦¬
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "content": result.get("content", ""),
                    "score": result.get("score", 0),
                    "raw_content": result.get("raw_content", "")
                })
            
            return formatted_results
        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {query}, ì—ëŸ¬: {e}")
            return []


class WebContentFetcher:
    """ì›¹ í˜ì´ì§€ ì „ì²´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    
    @staticmethod
    async def fetch_single_url(session, url: str, timeout: int = 10) -> tuple[str, str]:
        """ë‹¨ì¼ URL ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"  â†’ {url[:60]}...")
            async with session.get(url, timeout=timeout, ssl=False) as response:
                html = await response.text()
                print(f"    âœ… ì„±ê³µ ({len(html)}ì)")
                return url, html
        except asyncio.TimeoutError:
            print(f"    â° íƒ€ì„ì•„ì›ƒ (10ì´ˆ ì´ˆê³¼)")
            return url, ""
        except Exception as e:
            print(f"    âŒ ì‹¤íŒ¨: {type(e).__name__}")
            return url, ""
    
    @staticmethod
    async def fetch_multiple(urls: List[str], timeout: int = 10) -> List[str]:
        """ì—¬ëŸ¬ URLì˜ ë‚´ìš©ì„ ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        print(f"\nğŸ“¥ {len(urls)}ê°œ URL ë¬¸ì„œ ë¡œë”© ì‹œì‘...")
        
        try:
            # aiohttpë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ë” ë¹ ë¥´ê³  ì•ˆì •ì )
            async with aiohttp.ClientSession() as session:
                tasks = [WebContentFetcher.fetch_single_url(session, url, timeout) for url in urls]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            print("\nğŸ“ HTML â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
            documents = []
            for url, html in results:
                if html and not isinstance(html, Exception):
                    # ê°„ë‹¨í•œ HTML â†’ í…ìŠ¤íŠ¸ ë³€í™˜
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ ì œê±°
                    for script in soup(["script", "style"]):
                        script.decompose()
                    text = soup.get_text()
                    # ê³µë°± ì •ë¦¬
                    lines = (line.strip() for line in text.splitlines())
                    text = '\n'.join(line for line in lines if line)
                    documents.append(text)
            
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ\n")
            return documents
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    @staticmethod
    def fetch_single(url: str) -> str:
        """ë‹¨ì¼ URL ë™ê¸° ë°©ì‹"""
        print(f"ğŸ“„ ë‹¨ì¼ URL ë¡œë”©: {url[:50]}...")
        try:
            result = asyncio.run(WebContentFetcher.fetch_multiple([url]))
            return result[0] if result else ""
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {url}, {e}")
            return ""



def filter_reliable_sources(results: List[Dict]) -> List[Dict]:
    """ì‹ ë¢°ë„ ë†’ì€ ì†ŒìŠ¤ë§Œ í•„í„°ë§"""
    # ì‹ ë¢°ë„ ë†’ì€ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
    reliable_domains = [
        'bloomberg.com',
        'reuters.com',
        'ft.com',
        'wsj.com',
        'iea.org',
        'mckinsey.com',
        'bcg.com',
        'deloitte.com',
        'pwc.com',
        'tesla.com',
        'marketwatch.com',
        'cnbc.com',
        'insideevs.com',
        'electrek.co',
        'cleantechnica.com'
    ]
    
    # í•œêµ­ ì‹ ë¢° ì†ŒìŠ¤
    korean_domains = [
        'mk.co.kr',
        'hankyung.com',
        'chosun.com',
        'joongang.co.kr',
        'etnews.com'
    ]
    
    all_reliable = reliable_domains + korean_domains
    
    filtered = []
    for result in results:
        url = result.get('url', '').lower()
        if any(domain in url for domain in all_reliable):
            filtered.append(result)
    
    # ì‹ ë¢° ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ë°˜í™˜
    if not filtered:
        return sorted(results, key=lambda x: x.get('score', 0), reverse=True)[:5]
    
    return filtered


def deduplicate_results(results: List[Dict]) -> List[Dict]:
    """ì¤‘ë³µ URL ì œê±°"""
    seen_urls = set()
    unique = []
    
    for result in results:
        url = result.get('url')
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique.append(result)
    
    return unique


@tool
def search_ev_market(query: str) -> str:
    """
    ì „ê¸°ì°¨ ì‹œì¥ ê´€ë ¨ ì •ë³´ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    """
    searcher = TavilySearchTool(max_results=settings.TAVILY_MAX_RESULTS)  # âœ… settings ì‚¬ìš©
    results = searcher.search_web(query)
    
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ê²°ê³¼ ìš”ì•½
    summary = f"'{query}' ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê±´:\n\n"
    for i, result in enumerate(results, 1):
        summary += f"{i}. {result['title']}\n"
        summary += f"   {result['content'][:200]}...\n"
        summary += f"   ì¶œì²˜: {result['url']}\n\n"
    
    return summary